The safety test by the Dawn Project represents the latest in a series of claims about the safety of the world’s leading electric carmaker. During the video, the latest version of Tesla Full Self-Driving (FSD) Beta software appears to repeatedly hit a stationary, child-sized mannequin right in front of it at an average speed of 25 mph. The Dawn Project founder Dan O’Dowd described the results as “deeply disturbing,” adding: “Elon Musk says Tesla’s Full Self-Driving software is ‘amazing’. “It’s not. It’s a lethal threat to all Americans. Over 100,000 Tesla drivers are already using the car’s Full Self-Driving mode on public roads, putting children at great risk in communities across the country.” Sharing the results on Twitter, Mr O’Dowd added that the test discovered the cars “will indiscriminately mow down children.” The Dawn Project’s claims form part of their ad campaign in which they aim to garner public support to pressure Congress to ban Tesla’s auto-driving technology. Mr O’Dowd argued that the test results show the need to prohibit self-driving cars until Tesla proves the vehicles “will not mow down children in crosswalks”. The safety test was carried out at the Willow Springs International Raceway and test track in Rosamond, California on June 21. The US National Highway Traffic Safety Administration (NHTSA) confirmed that it “currently has an open and active investigation of Tesla's Autopilot active driver assistance system”. A spokesperson said this included the full self-driving software and that the agency “considers all relevant data and information that may assist its investigation”. The organisation confirmed in June that it was expanding an investigation into 830,000 Tesla cars across all four current model lines, after an analysis of a number of accidents revealed patterns in the car’s performance and driver behaviour. READ MORE: Elon Musk’s feud with Twitter continues [REVEAL] The NHTSA said the widened investigation would aim to examine the degree to which Tesla’s autopilot technology and associated systems “may exacerbate human factors or behavioural safety risks by undermining the effectiveness of the driver’s supervision”. Since 2016, the agency has investigated 30 crashes involving Teslas equipped with automated driving systems, 19 of them fatal. In February this year, Tesla recalled nearly 54,000 cars and SUVs because their full self-driving software was found to let them roll through stop signs without coming to a complete halt. Documents shared by US safety regulators said Tesla had agreed to the recall after two meetings with officials from NHTSA. Tesla said at the time that it was not aware of any crashes or injuries caused by the feature, adding that there had been no warranty claims as a result of issues with the software. DON'T MISS: Elon Musk was spat on by Russian during clash over rocket designs [REVEAL]Elon Musk risks 'downfall of empire' if Twitter wins £36bn mega-deal [INSIGHT]Elon Musk denies affair with Google founder Sergey Brin's wife [ANALYSIS] Meanwhile, in nearly 400 crashes in the US involving cars with driver-assist systems reported by automakers between July 2021 and this past May, more Teslas were involved than all other manufacturers combined. However, Mr O’Dowd has himself drawn accusations that he is motivated by competition with Tesla, as his company claims to be an expert in making software used by automated driving systems. He has insisted his Green Hills software doesn’t compete with the electric car giant, saying he doesn’t make self-driving cars. He has acknowledged however that some car companies use his company’s software for certain components. Express.co.uk has approached Tesla for comment.