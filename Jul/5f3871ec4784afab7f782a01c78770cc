 The Government department began trialling the use of a machine learning algorithm to detect cases of benefits fraud over the last year. The algorithm analyses historical data to predict which cases are likely to be fraudulent in the future, without being explicitly programmed by a human. The plans were outlined in the DWP’s annual report and accounts and were also confirmed in a report by the National Audit Office (NAO) which was published on Thursday 7 July.     While campaigners have welcomed the crackdown on fraud, many have highlighted that the algorithm could mean that some people could be discriminated against with their claims being classed as “potentially fraudulent”. Ariane Adam, legal director of the Public Law Project said: “Despite many requests under the Freedom of Information Act, the DWP has previously refused to provide details about its use of automation to assess Universal Credit applications. This lack of transparency is very problematic. “Without transparency, there can be no evaluation, and without evaluation, it is not possible to tell if a system works reliably, lawfully or fairly.” Ms Adam said that the “discriminatory impact” was a “massive risk” for marginalised or vulnerable groups. READ MORE: How much are disability payments and what health conditions qualify?  She added: “This could be, for example, because the historic data may be inaccurate or because it may be tainted by human bias that will be exacerbated by the machine." In 2021-22 the model has been run to detect fraud in advanced claims already in payment. The DWP now plans to trial the model on claims before any payment has been made in 2022-23. In its annual report, the DWP stated: “If successful this could improve its ability to prevent fraud before these benefits are paid out, avoiding the need to seek recovery.” The NAO’s report did note that the DWP intends to monitor the model for unintended bias and is aware that groups with protected characteristics are disproportionately impacted and that the model could stop claimants from receiving their money.      The DWP report said: “It is unavoidable that some cases flagged as potentially fraudulent will turn out to be legitimate claims." “If the model were to disproportionately identify a group with a protected characteristic as more likely to commit fraud, the model could inadvertently obstruct fair access to benefits.” It also noted that the final decision on potential fraudulent cases would be made at the discretion of a DWP caseworker. Ms Adam also added that as the UK was “in the midst cost-of-living crisis”, the severity of Britons having their benefits stopped before they were even paid out “because a computer algorithm said 'no' was very problematic”. READ MORE: Scam Warning: Victims losing more than £36,000 on average     She said: “Departments across Government need to commit to a great deal more than just being aware of the risks. “We need a clear commitment that all Government departments will be transparent about how they use algorithms. “The presumption should be that detailed information about how automated decision-making tools work is made available and any data and analysis gathered from trials is published without charities having to make endless Freedom of Information requests. “Any exemptions to this presumption must be justified by Government and be necessary and proportionate. The exemption should not be the default.”     Responding to the criticisms, the DWP said that it “did not use artificial intelligence to make decisions on how a Universal Credit claim should progress.” It also said that it was going to “continue to work hard” to be as “transparent as possible” about its claims process “without compromising our ability to identify fraud". A DWP spokesperson said: “It is right that we keep up with fraud in today’s digital age so we can prevent, detect and deter those who would try to cheat the system and more importantly, improve our support for genuine claimants.”   